
@article{bonjour_decision_2022,
	title = {Decision {Making} in {Monopoly} {Using} a {Hybrid} {Deep} {Reinforcement} {Learning} {Approach}},
	volume = {6},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2471-285X},
	url = {https://ieeexplore.ieee.org/document/9775710/},
	doi = {10.1109/TETCI.2022.3166555},
	abstract = {Learning to adapt and make real-time informed decisions in a dynamic and complex environment is a challenging problem. Monopoly is a popular strategic board game that requires players to make multiple decisions during the game. Decision-making in Monopoly involves many real-world elements such as strategizing, luck, and modeling of opponent’s policies. In this paper, we present novel representations for the state and action space for the full version of Monopoly and define an improved reward function. Using these, we show that our deep reinforcement learning agent can learn winning strategies for Monopoly against different fixed-policy agents. In Monopoly, players can take multiple actions even if it is not their. turn to roll the dice. Some of these actions occur more frequently than others, resulting in a skewed distribution that adversely affects the performance of the learning agent. To tackle the non-uniform distribution of actions, we propose a hybrid approach that combines deep reinforcement learning (for frequent but complex decisions) with a fixed-policy approach (for infrequent but straightforward decisions). We develop learning agents using proximal policy optimization (PPO) and double deep Q-learning (DDQN) algorithms and compare the standard approach to our proposed hybrid approach. Experimental results show that our hybrid agents outperform standard agents by 20\% in the number of games won against fixed-policy agents. The hybrid PPO agent performs the best with a win rate of 91\% against fixed-policy agents.},
	number = {6},
	urldate = {2025-09-13},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Bonjour, Trevor and Haliem, Marina and Alsalem, Aala and Thomas, Shilpa and Li, Hongyu and Aggarwal, Vaneet and Kejriwal, Mayank and Bhargava, Bharat},
	month = dec,
	year = {2022},
	pages = {1335--1344},
	file = {Submitted Version:/Users/tbonjour/Zotero/storage/RSFXPBZE/Bonjour et al. - 2022 - Decision Making in Monopoly Using a Hybrid Deep Reinforcement Learning Approach.pdf:application/pdf},
}

@inproceedings{bonjour_information_2022,
	series = {{UAI} 22},
	title = {Information theoretic approach to detect collusion in multi-agent games},
	url = {https://proceedings.mlr.press/v180/bonjour22a.html},
	urldate = {2025-09-13},
	booktitle = {Uncertainty in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Bonjour, Trevor and Aggarwal, Vaneet and Bhargava, Bharat},
	year = {2022},
	pages = {223--232},
	file = {Available Version (via Google Scholar):/Users/tbonjour/Zotero/storage/LK42L5PY/Bonjour et al. - 2022 - Information theoretic approach to detect collusion in multi-agent games.pdf:application/pdf},
}

@incollection{boult_multi-agent_2024,
	address = {Cham},
	title = {Multi-agent {Game} {Domain}: {Monopoly}},
	isbn = {978-3-031-33053-7 978-3-031-33054-4},
	shorttitle = {Multi-agent {Game} {Domain}},
	url = {https://link.springer.com/10.1007/978-3-031-33054-4_7},
	language = {en},
	urldate = {2025-09-13},
	booktitle = {A {Unifying} {Framework} for {Formal} {Theories} of {Novelty}},
	publisher = {Springer Nature Switzerland},
	author = {Bonjour, Trevor and Haliem, Marina and Aggarwal, Vaneet and Kejriwal, Mayank and Bhargava, Bharat},
	editor = {Boult, Terrance and Scheirer, Walter},
	year = {2024},
	doi = {10.1007/978-3-031-33054-4_7},
	note = {Series Title: Synthesis Lectures on Computer Vision},
	pages = {97--105},
}

@inproceedings{mehta_enhancing_2025,
	title = {Enhancing {Extrusion} with {AI}: {Leveraging} {Machine} {Learning} for {Texture} {Properties} {Prediction} of {Extruded} {High}-{Moisture} {Meat} {Analogs}},
	shorttitle = {Enhancing {Extrusion} with {AI}},
	url = {https://aiche.confex.com/aiche/2025/meetingapp.cgi/Paper/717309},
	urldate = {2025-09-13},
	booktitle = {2025 {AIChE} {Annual} {Meeting}},
	publisher = {AIChE},
	author = {Mehta, Halak and Bonjour, Trevor and Mishra, Dharmendra K.},
	month = nov,
	year = {2025},
}

@inproceedings{baghban_karimi_exploring_2025,
	address = {New York, NY, USA},
	series = {{ITiCSE} 2025},
	title = {Exploring {Effective} {Early} {Research} {Exposure} for {Broadening} {Participation} in {Computing} {Science}},
	volume = {2},
	isbn = {979-8-4007-1569-3},
	url = {https://doi.org/10.1145/3724389.3731278},
	doi = {10.1145/3724389.3731278},
	abstract = {Evidence supports offering research experiences for undergraduate computing science students as a means of broadening participation in computing[7, 10, 11]. However, student perceptions about computing science research, how students become interested in these research experiences, and the details of effective design and delivery of programs capable of attracting and retaining this interest are less explored. In this study, we investigate the design and delivery of undergraduate research programs. We expand on and explore several factors, including but not limited to cultural relevance, the presence of a cross-disciplinary high-level view, task assignment, entry point, and support elements of a program.},
	urldate = {2025-09-13},
	booktitle = {Proceedings of the 30th {ACM} {Conference} on {Innovation} and {Technology} in {Computer} {Science} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Baghban Karimi, Ouldooz and Robinson, Rebecca and Reckinger, Shanon and Alberini, Giulia and Bhagavatula, Sruti and Bonjour, Trevor and Hmeljak, Dimitrij (Mitja) and Liaskos, Konstantinos and Rodger, Susan H. and Sembey, Ruchi and Venn-Wycherley, Megan},
	month = jun,
	year = {2025},
	pages = {695--696},
}

@article{islam_q-learning_2024,
	title = {A {Q}-learning {Novelty} {Search} {Strategy} for {Evaluating} {Robustness} of {Deep} {Reinforcement} {Learning} in {Open}-world {Environments}},
	issn = {1941-1294},
	url = {https://ieeexplore.ieee.org/abstract/document/10697140},
	doi = {10.1109/MIS.2024.3469574},
	abstract = {Despite substantial progress in deep reinforcement learning (DRL), a systematic characterization of DRL agents’ robustness to unexpected events in the environment is relatively under-studied. Such unexpected events (“novelties”), especially those that are more structural than parametric, may significantly deteriorate the performance of DRL agents, leading them to be unfit for open-world environments and applications. However, not all novelties affect an agent’s performance equally. Unfortunately, even with reasonable and constrained definitions of the problem, the space of all novelties can be (at least) exponential. Hence, an effective search strategy is required to find novelties that can adversely affect the agent. This paper presents a formalism for this problem and proposes a deep Q-learning-based novelty-search strategy that efficiently and systematically finds candidate (potentially complex) novelties with significant negative impact on a DRL agent. We conduct a detailed set of experiments in a stochastic multi-agent game environment (Monopoly) with complex decision-making properties.},
	urldate = {2025-09-13},
	journal = {IEEE Intelligent Systems},
	author = {Islam, Shafkat and Chiu, Min-Hsueh and Bonjour, Trevor and de Oliveira, Ruy and Bhargava, Bharat and Kejriwal, Mayank},
	year = {2024},
	keywords = {Deep reinforcement learning, Games, Heating systems, Intelligent systems, Monopoly, Robustness, Search problems, Systematics, Terminology, Training},
	pages = {1--10},
}
